{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20405296",
   "metadata": {},
   "source": [
    "Analyzing the responses to a hacker news article \"who is looking for a job\"\n",
    "\n",
    "Used Gemini, had to iterate twice \n",
    "- escape the html properly\n",
    "- specify the fields explicitly\n",
    "\n",
    "Lot's more could be done, but pretty straightforward for the output here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd665be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import html # For unescaping HTML entities\n",
    "from bs4 import BeautifulSoup # For parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cea993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_item_ids(num_items=500):\n",
    "    \"\"\"Fetches the IDs of the top stories from Hacker News.\"\"\"\n",
    "    url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[:num_items]\n",
    "\n",
    "def get_item_details(item_id):\n",
    "    \"\"\"Fetches the details of a specific item (story or comment) by its ID.\"\"\"\n",
    "    url = f\"https://hacker-news.firebaseio.com/v0/item/{item_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def find_who_is_hiring_thread_id(top_item_ids):\n",
    "    \"\"\"Tries to find the 'Who is hiring?' or 'Who wants to be hired?' thread among the top stories.\"\"\"\n",
    "    for item_id in top_item_ids:\n",
    "        item = get_item_details(item_id)\n",
    "        if item and 'title' in item and (\"Who is hiring?\" in item['title'] or \"Who wants to be hired?\" in item['title']):\n",
    "            print(f\"Found thread: '{item['title']}' with ID: {item_id}\")\n",
    "            return item_id\n",
    "    return None\n",
    "\n",
    "def extract_links_from_text(text):\n",
    "    \"\"\"\n",
    "    Extracts various types of links from a given text,\n",
    "    first unescaping HTML entities and then parsing <a> tags.\n",
    "    \"\"\"\n",
    "    website = None\n",
    "    resume = None\n",
    "    github = None\n",
    "\n",
    "    # 1. Unescape HTML entities\n",
    "    unescaped_text = html.unescape(text)\n",
    "\n",
    "    # 2. Use BeautifulSoup to parse the HTML and find all <a> tags\n",
    "    soup = BeautifulSoup(unescaped_text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    for link_tag in links:\n",
    "        url = link_tag['href']\n",
    "\n",
    "        if \"github.com\" in url:\n",
    "            github = url\n",
    "        elif \"resume\" in url.lower() or \"cv\" in url.lower() or any(ext in url.lower() for ext in ['.pdf', '.doc', '.docx']):\n",
    "            resume = url\n",
    "        else:\n",
    "            if website is None: # Only capture the first general website link found\n",
    "                website = url\n",
    "    return website, resume, github\n",
    "\n",
    "def parse_structured_info(comment_text):\n",
    "    \"\"\"\n",
    "    Parses specific fields like Location and Technologies from the comment text\n",
    "    based on the expected structure.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'Location': None,\n",
    "        'Remote': None,\n",
    "        'Willing to relocate': None,\n",
    "        'Technologies': None,\n",
    "        'Résumé/CV': None, # We'll still extract this from <a> tags for consistency\n",
    "        'Email': None,\n",
    "    }\n",
    "\n",
    "    # Unescape HTML entities first, as the structure might also contain them\n",
    "    unescaped_text = html.unescape(comment_text)\n",
    "\n",
    "    # Split the text by <p> tags or newlines to process line by line\n",
    "    # Using regex to split by <p> (case-insensitive) or newline characters\n",
    "    lines = re.split(r'(?:<p>|\\n)', unescaped_text)\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"Location:\"):\n",
    "            data['Location'] = line.split(\"Location:\", 1)[1].strip()\n",
    "        elif line.startswith(\"Remote:\"):\n",
    "            data['Remote'] = line.split(\"Remote:\", 1)[1].strip()\n",
    "        elif line.startswith(\"Willing to relocate:\"):\n",
    "            data['Willing to relocate'] = line.split(\"Willing to relocate:\", 1)[1].strip()\n",
    "        elif line.startswith(\"Technologies:\"):\n",
    "            data['Technologies'] = line.split(\"Technologies:\", 1)[1].strip()\n",
    "        elif line.startswith(\"Résumé/CV:\"):\n",
    "            # We will still rely on extract_links_from_text for the actual URL\n",
    "            # but capture this if present for contextual understanding\n",
    "            pass # The link extraction will handle the URL\n",
    "        elif line.startswith(\"Email:\"):\n",
    "            data['Email'] = line.split(\"Email:\", 1)[1].strip()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3757d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hacker_news_thread(thread_id):\n",
    "    \"\"\"\n",
    "    Analyzes a Hacker News thread to extract information from comments.\n",
    "    \"\"\"\n",
    "    thread_details = get_item_details(thread_id)\n",
    "    if not thread_details or 'kids' not in thread_details:\n",
    "        print(\"Could not find comments for this thread.\")\n",
    "        return\n",
    "\n",
    "    job_responses = []\n",
    "    total_responses = 0\n",
    "    with_website = 0\n",
    "    with_resume_link = 0\n",
    "    with_github_repo = 0\n",
    "\n",
    "    location_counts = defaultdict(int)\n",
    "    technology_counts = defaultdict(int)\n",
    "\n",
    "    # Let's track the raw data for locations and technologies for more granular analysis later\n",
    "    raw_locations = []\n",
    "    raw_technologies = []\n",
    "\n",
    "\n",
    "    for comment_id in thread_details['kids']:\n",
    "        comment = get_item_details(comment_id)\n",
    "        if comment and 'text' in comment:\n",
    "            total_responses += 1\n",
    "            comment_text = comment['text']\n",
    "\n",
    "            # Extract structured info (Location, Technologies)\n",
    "            structured_info = parse_structured_info(comment_text)\n",
    "\n",
    "            # Extract links (Website, Resume, Github)\n",
    "            website, resume_link, github_repo = extract_links_from_text(comment_text)\n",
    "\n",
    "            if website:\n",
    "                with_website += 1\n",
    "            if resume_link:\n",
    "                with_resume_link += 1\n",
    "            if github_repo:\n",
    "                with_github_repo += 1\n",
    "\n",
    "            # Populate location and technology counts\n",
    "            if structured_info['Location']:\n",
    "                # Basic cleanup, e.g., splitting by comma for multiple locations if desired\n",
    "                locations = [loc.strip() for loc in structured_info['Location'].split(',')]\n",
    "                for loc in locations:\n",
    "                    if loc: # Ensure not empty string\n",
    "                        location_counts[loc.lower()] += 1 # Standardize to lowercase\n",
    "                        raw_locations.append(loc)\n",
    "\n",
    "            if structured_info['Technologies']:\n",
    "                # Split by comma and trim whitespace\n",
    "                technologies = [tech.strip() for tech in structured_info['Technologies'].split(',')]\n",
    "                for tech in technologies:\n",
    "                    if tech: # Ensure not empty string\n",
    "                        technology_counts[tech.lower()] += 1 # Standardize to lowercase\n",
    "                        raw_technologies.append(tech)\n",
    "\n",
    "\n",
    "            job_responses.append({\n",
    "                'id': comment_id,\n",
    "                'text': comment_text,\n",
    "                'website': website,\n",
    "                'resume_link': resume_link,\n",
    "                'github_repo': github_repo,\n",
    "                'parsed_location': structured_info['Location'],\n",
    "                'parsed_remote': structured_info['Remote'],\n",
    "                'parsed_relocate': structured_info['Willing to relocate'],\n",
    "                'parsed_technologies': structured_info['Technologies'],\n",
    "                'parsed_email': structured_info['Email']\n",
    "            })\n",
    "\n",
    "            # For comments without GitHub, check website/resume (this requires fetching content from those links)\n",
    "            # This logic remains a placeholder for advanced implementation\n",
    "            if not github_repo and (website or resume_link):\n",
    "                pass\n",
    "\n",
    "    print(f\"\\n--- Analysis Summary ({total_responses} responses) ---\")\n",
    "    print(f\"Responses with website: {with_website} ({with_website/total_responses:.2%})\")\n",
    "    print(f\"Responses with resume link: {with_resume_link} ({with_resume_link/total_responses:.2%})\")\n",
    "    print(f\"Responses with GitHub repo: {with_github_repo} ({with_github_repo/total_responses:.2%})\")\n",
    "\n",
    "    print(\"\\n--- Top Locations ---\")\n",
    "    # Sort and print top locations\n",
    "    sorted_locations = sorted(location_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    for loc, count in sorted_locations[:10]: # Print top 10\n",
    "        print(f\"{loc.title()}: {count}\") # Capitalize for display\n",
    "\n",
    "    print(\"\\n--- Top Technologies ---\")\n",
    "    # Sort and print top technologies\n",
    "    sorted_technologies = sorted(technology_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    for tech, count in sorted_technologies[:15]: # Print top 15\n",
    "        print(f\"{tech.title()}: {count}\")\n",
    "\n",
    "    return job_responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff87bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Fetching top Hacker News stories...\")\n",
    "    top_ids = get_top_item_ids(num_items=500) # Get more IDs to increase chances of finding the thread\n",
    "    who_is_hiring_thread_id = find_who_is_hiring_thread_id(top_ids)\n",
    "\n",
    "    if who_is_hiring_thread_id:\n",
    "        print(f\"Analyzing thread ID: {who_is_hiring_thread_id}\")\n",
    "        job_data = analyze_hacker_news_thread(who_is_hiring_thread_id)\n",
    "        # You can now further process job_data, e.g., save to CSV, perform deeper analysis.\n",
    "    else:\n",
    "        print(\"Could not find a 'Who is hiring?' or 'Who wants to be hired?' thread among the top stories.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
